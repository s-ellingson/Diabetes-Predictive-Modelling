{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e01dd238",
   "metadata": {},
   "source": [
    "# ISYE 6740 - Project\n",
    "### Natural Language Processing Methods with Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be28f839",
   "metadata": {},
   "source": [
    "### Step 0. Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d465553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import collections # used to identify counts of words\n",
    "import pandas as pd # used to load in dataset\n",
    "import numpy as np\n",
    "from keras.layers import TextVectorization # for word tokenization\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cc548f",
   "metadata": {},
   "source": [
    "### Step 1. Load and Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2cf4209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Norwegian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Let's try something.</td>\n",
       "      <td>La oss prøve noe.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have to go to sleep.</td>\n",
       "      <td>Jeg må legge meg.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Today is June 18th and it is Muiriel's birthday!</td>\n",
       "      <td>I dag er det juni den 18. og det er Muiriels b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Muiriel is 20 now.</td>\n",
       "      <td>Muiriel er 20 nå.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The password is \"Muiriel\".</td>\n",
       "      <td>Passordet er \"Muiriel\".</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            English  \\\n",
       "0                              Let's try something.   \n",
       "1                            I have to go to sleep.   \n",
       "2  Today is June 18th and it is Muiriel's birthday!   \n",
       "3                                Muiriel is 20 now.   \n",
       "4                        The password is \"Muiriel\".   \n",
       "\n",
       "                                           Norwegian  \n",
       "0                                  La oss prøve noe.  \n",
       "1                                  Jeg må legge meg.  \n",
       "2  I dag er det juni den 18. og det er Muiriels b...  \n",
       "3                                  Muiriel er 20 nå.  \n",
       "4                            Passordet er \"Muiriel\".  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df = (pd.read_csv('pairs.tsv', sep='\\t', header=None)).drop([0, 2], axis=1)\n",
    "words_df.columns = ['English', 'Norwegian']\n",
    "\n",
    "words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f42465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13066, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c00a1c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total English Words: 82687\n",
      "Unique English Words: 6704\n",
      "Top 3 English Words: [('the', 3195), ('i', 2872), ('to', 2462)]\n",
      "\n",
      "Total Norwegian Words: 81460\n",
      "Unique Norwegian Words: 8338\n",
      "Top 3 Norwegian Words: [('jeg', 3663), ('er', 3335), ('det', 2094)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get total, unique counts as well as most frequent words for each language\n",
    "for lang in words_df.columns:\n",
    "    counts_df = words_df.copy() # create new df for string editing\n",
    "    counts_df[lang] = counts_df[lang].str.replace(r'[^\\w\\s]+', '', regex=True) # removes all punctuation\n",
    "    \n",
    "    # use Counter function to get counts of each word\n",
    "    results = collections.Counter()\n",
    "    counts_df[lang].str.lower().str.split().apply(results.update)\n",
    "    \n",
    "    # print results\n",
    "    print(f'Total {lang} Words: {sum(results.values())}')\n",
    "    print(f'Unique {lang} Words: {len(results.keys())}')\n",
    "    print(f'Top 3 {lang} Words: {results.most_common(3)}\\n')\n",
    "  \n",
    "    counts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c494732b",
   "metadata": {},
   "source": [
    "We can see that the total words for the English language in this dataset is only slighly higher than its Norwegian counterpart data; however, Norway has 1,634 more unique words. Another interesting insight from the information gathered here is that the words 'jeg', 'er', and 'det' are the most popular Norwegian words. Intuitively, this makes sense; a neat little thing about Norwegian is that 'er' serves as a linking verb for both singular and plural nouns. For example:\n",
    "- \"He is\" = \"Han er\"\n",
    "- \"They are\" = \"De er\"\n",
    "- \"I am\" = \"Jeg er\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8be87e",
   "metadata": {},
   "source": [
    "### Step 2. Tokenize and Pad Data for Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bbb197",
   "metadata": {},
   "source": [
    "No cleaning is needed with the dataset, as the raw data was already in grammatically correct fashion. While we removed punctuation for the data inspection phase to get true counts of all words, that missing information may provide further syntactic insight and help the models we create in the future. For that reason, we leave punctuation characters within the dataset.\n",
    "\n",
    "Our next step is to tokenize the data, which for each language, creates a unique numeric identifier for each word. This is a necessary step before we can feed our language data into the models we make. With the TextVectorization function from the Keras package, we map integer values to all the words in the dataset for each language (separately). \n",
    "\n",
    "Another step we must employ is a technique called padding. Since NLP models require all inputs to be of the same length, there is an incongruency with how our data is currently shaped due to sentences having different word counts. What padding does is add values to the shorter sentences in order to have the same value count as the longest sentence in the dataset. A method often used in tandem with this is truncating, which is cutting down the maximum amount of words allowed in a particular sentence. Since we lose data this way, we will prepare our data only with padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bc9508d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409/409 [==============================] - 0s 633us/step\n",
      "English Token Array Shape: (13066, 209)\n",
      "Length of Longest Sentence: 209\n",
      "409/409 [==============================] - 0s 648us/step\n",
      "Norwegian Token Array Shape: (13066, 213)\n",
      "Length of Longest Sentence: 213\n"
     ]
    }
   ],
   "source": [
    "# Make vectorization layer.\n",
    "vectorize_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=None, # no limit on max tokens\n",
    "    standardize='lower', # standardize lowercase (no stripping of punctuation)\n",
    "    split='whitespace', # splitting on space between words\n",
    "    output_mode='int', # getting integer values for tokens\n",
    ")\n",
    "\n",
    "lang_dict = {}\n",
    "lang_vocab = {}\n",
    "for lang in words_df.columns:\n",
    "    # Adapt layer on the text-only dataset to create the vocabulary.\n",
    "    vectorize_layer.adapt(words_df[lang])\n",
    "    lang_vocab[lang] = vectorize_layer.get_vocabulary(include_special_tokens = False)\n",
    "    \n",
    "    # Create the model that uses the text layer.\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    # Start by creating an explicit input layer. It needs to have a shape of\n",
    "    # (1,) (because we need to guarantee that there is exactly one string\n",
    "    # input per batch), and the dtype needs to be 'string'.\n",
    "    model.add(keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "    # The first layer in our model is the vectorization layer. After this\n",
    "    # layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "    # indices.\n",
    "    model.add(vectorize_layer)\n",
    "\n",
    "    # Get token value array.\n",
    "    input_data = words_df[lang]\n",
    "    lang_dict[lang] = np.array(model.predict(input_data).to_tensor())\n",
    "    \n",
    "    # Detail the shape of that array.\n",
    "    max_list = max(lang_dict[lang], key = lambda i: len(i))\n",
    "    max_len = len(max_list)\n",
    "    print(f'{lang} Token Array Shape: {lang_dict[lang].shape}\\nLength of Longest Sentence: {max_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4820f0bb",
   "metadata": {},
   "source": [
    "As expected, both token arrays have the same length as the original dataset (one list for each sentence). We also see that the maximum sentence lengths for English and Norwegian are 209 and 213 words, respectively. To double check that there was no error in the padding process, we have added a list at the bottom of the previous code cell to calculate and return the vocabulary array entry with the longest length. In doing so, we have confirmed that the padding process was successful; we now have two arrays with tokenized values for each language, which will be used for modelling.\n",
    "\n",
    "To make sure our models perform correctly, we need both token arrays to have the same dimensions. This requires additional padding for the English array to match the Norwegian array's size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19177a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_x = np.pad(lang_dict[\"English\"], ((0,0),(0,4)), 'constant') # pad English array to Norwegian array size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d40e2f",
   "metadata": {},
   "source": [
    "A final step is to make a function that we can apply to each model. Output is in the form of IDs, so we want to get this back to text. The function below does just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9de9479e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e9a39",
   "metadata": {},
   "source": [
    "### Step 3. Use SMT and NMT techniques to make models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6184cd91",
   "metadata": {},
   "source": [
    "##### Model 1. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3346070",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# get shape details of token array length and word indeces\n",
    "max_norsk_sequence_length = lang_dict[\"Norwegian\"].shape[1]\n",
    "english_vocab_size = np.unique(lang_dict[\"English\"]).shape[0]\n",
    "norsk_vocab_size = np.unique(lang_dict[\"Norwegian\"]).shape[0]\n",
    "\n",
    "\n",
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, norsk_vocab_size):\n",
    "    learning_rate = 1e-3\n",
    "    input_seq = Input(input_shape[1:])\n",
    "    rnn = GRU(64, return_sequences = True)(input_seq)\n",
    "    logits = TimeDistributed(Dense(norsk_vocab_size))(rnn)\n",
    "    model = Model(input_seq, Activation('softmax')(logits))\n",
    "    model.compile(loss = sparse_categorical_crossentropy, \n",
    "                 optimizer = Adam(learning_rate), \n",
    "                 metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#tests.test_simple_model(simple_model)\n",
    "\n",
    "#tmp_x = pad(lang_dict['English'], lang_dict['Norwegian'].shape[1]) # padding english token vectors to max norwegian\n",
    "tmp_x = tmp_x.reshape((lang_dict[\"Norwegian\"].shape[0], lang_dict[\"Norwegian\"].shape[1], 1)) # reshape english token vector\n",
    "\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_norsk_sequence_length,\n",
    "    english_vocab_size,\n",
    "    norsk_vocab_size)\n",
    "simple_rnn_model.fit(tmp_x, lang_dict[\"Norwegian\"], batch_size=1024, epochs=10, validation_split=0.2)\n",
    "# Print prediction(s)\n",
    "#print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a29c79",
   "metadata": {},
   "source": [
    "### Step 4. Compare model accuracies (summary section)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
